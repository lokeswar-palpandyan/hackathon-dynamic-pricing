name: Model Retraining Pipeline

on:
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      force_retrain:
        description: 'Force retraining regardless of triggers'
        required: false
        default: 'false'
        type: boolean
      data_source:
        description: 'Data source for retraining'
        required: false
        default: 'production'
        type: choice
        options:
          - production
          - staging
          - manual_upload
  repository_dispatch:
    types: [model-performance-degraded, data-drift-detected]

env:
  PYTHON_VERSION: '3.9'
  DOCKER_REGISTRY: 'ghcr.io'
  MODEL_REGISTRY: 'mlflow'
  AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
  AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
  AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
  AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}

jobs:
  check-retraining-triggers:
    name: Check Retraining Triggers
    runs-on: ubuntu-latest
    outputs:
      should_retrain: ${{ steps.check-triggers.outputs.should_retrain }}
      trigger_reason: ${{ steps.check-triggers.outputs.trigger_reason }}
      data_version: ${{ steps.check-triggers.outputs.data_version }}
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install mlflow boto3 azure-storage-blob pandas numpy scikit-learn
        
    - name: Check retraining triggers
      id: check-triggers
      run: |
        python -c "
        import os
        import sys
        import json
        from datetime import datetime, timedelta
        import pandas as pd
        
        # Initialize trigger flags
        should_retrain = False
        trigger_reasons = []
        
        # Check if force retrain is requested
        force_retrain = '${{ github.event.inputs.force_retrain }}' == 'true'
        if force_retrain:
            should_retrain = True
            trigger_reasons.append('Manual force retrain requested')
        
        # Check for repository dispatch events (performance degradation, data drift)
        if '${{ github.event_name }}' == 'repository_dispatch':
            should_retrain = True
            trigger_reasons.append(f'Repository dispatch: ${{ github.event.action }}')
        
        # Check data freshness (simulate checking data age)
        try:
            # In a real scenario, this would check your data warehouse/lake
            current_time = datetime.now()
            last_training_time = current_time - timedelta(days=7)  # Simulate last training was 7 days ago
            
            if (current_time - last_training_time).days >= 7:
                should_retrain = True
                trigger_reasons.append('Data is older than 7 days')
        except Exception as e:
            print(f'Warning: Could not check data freshness: {e}')
        
        # Check model performance metrics (simulate checking model drift)
        try:
            # In a real scenario, this would query your monitoring system
            # Simulate performance check
            performance_threshold = 0.85
            current_performance = 0.82  # Simulate degraded performance
            
            if current_performance < performance_threshold:
                should_retrain = True
                trigger_reasons.append(f'Model performance below threshold: {current_performance} < {performance_threshold}')
        except Exception as e:
            print(f'Warning: Could not check model performance: {e}')
        
        # Generate data version
        data_version = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Output results
        print(f'Should retrain: {should_retrain}')
        print(f'Trigger reasons: {trigger_reasons}')
        print(f'Data version: {data_version}')
        
        # Set GitHub outputs
        print(f'::set-output name=should_retrain::{str(should_retrain).lower()}')
        print(f'::set-output name=trigger_reason::{\";\".join(trigger_reasons) if trigger_reasons else \"No triggers detected\"}')
        print(f'::set-output name=data_version::{data_version}')
        "

  collect-training-data:
    name: Collect Training Data
    runs-on: ubuntu-latest
    needs: check-retraining-triggers
    if: needs.check-retraining-triggers.outputs.should_retrain == 'true'
    outputs:
      data_path: ${{ steps.collect-data.outputs.data_path }}
      data_size: ${{ steps.collect-data.outputs.data_size }}
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install azure-storage-blob pandas numpy
        
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
        
    - name: Collect training data
      id: collect-data
      run: |
        python -c "
        import os
        import pandas as pd
        from datetime import datetime, timedelta
        
        # Simulate data collection from various sources
        print('Collecting training data...')
        
        # In a real scenario, this would:
        # 1. Query production database for recent transactions
        # 2. Fetch external data (competitor prices, market data)
        # 3. Combine and clean the data
        # 4. Store in training data format
        
        # Simulate data collection
        data_source = '${{ github.event.inputs.data_source }}'
        end_date = datetime.now()
        start_date = end_date - timedelta(days=90)  # Last 90 days
        
        print(f'Collecting data from {start_date} to {end_date}')
        print(f'Data source: {data_source}')
        
        # Create mock training data
        import numpy as np
        n_samples = 10000
        
        training_data = pd.DataFrame({
            'product_id': np.random.randint(1, 1000, n_samples),
            'base_price': np.random.uniform(10, 1000, n_samples),
            'competitor_price': np.random.uniform(8, 1200, n_samples),
            'demand_score': np.random.uniform(0, 1, n_samples),
            'inventory_level': np.random.randint(0, 1000, n_samples),
            'season_factor': np.random.uniform(0.8, 1.2, n_samples),
            'target_price': np.random.uniform(8, 1200, n_samples)
        })
        
        # Save training data
        data_path = f'data/training/training_data_{\"${{ needs.check-retraining-triggers.outputs.data_version }}\"}}.csv'
        os.makedirs('data/training', exist_ok=True)
        training_data.to_csv(data_path, index=False)
        
        data_size = os.path.getsize(data_path)
        
        print(f'Training data saved to: {data_path}')
        print(f'Data size: {data_size} bytes')
        print(f'Number of samples: {len(training_data)}')
        
        # Set GitHub outputs
        print(f'::set-output name=data_path::{data_path}')
        print(f'::set-output name=data_size::{data_size}')
        "
        
    - name: Upload training data artifact
      uses: actions/upload-artifact@v3
      with:
        name: training-data-${{ needs.check-retraining-triggers.outputs.data_version }}
        path: data/training/
        retention-days: 30

  retrain-model:
    name: Retrain Model
    runs-on: ubuntu-latest
    needs: [check-retraining-triggers, collect-training-data]
    if: needs.check-retraining-triggers.outputs.should_retrain == 'true'
    outputs:
      model_version: ${{ steps.train-model.outputs.model_version }}
      model_performance: ${{ steps.train-model.outputs.model_performance }}
      challenger_endpoint: ${{ steps.deploy-challenger.outputs.challenger_endpoint }}
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install mlflow scikit-learn xgboost lightgbm optuna
        
    - name: Download training data
      uses: actions/download-artifact@v3
      with:
        name: training-data-${{ needs.check-retraining-triggers.outputs.data_version }}
        path: data/training/
        
    - name: Train multiple models
      id: train-model
      run: |
        python -c "
        import os
        import mlflow
        import mlflow.sklearn
        import pandas as pd
        import numpy as np
        from sklearn.model_selection import train_test_split, cross_val_score
        from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
        from sklearn.linear_model import LinearRegression
        from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
        import pickle
        import json
        from datetime import datetime
        
        # Load training data
        data_path = '${{ needs.collect-training-data.outputs.data_path }}'
        df = pd.read_csv(data_path)
        
        print(f'Loaded {len(df)} samples for training')
        
        # Prepare features and target
        feature_cols = ['base_price', 'competitor_price', 'demand_score', 'inventory_level', 'season_factor']
        X = df[feature_cols]
        y = df['target_price']
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Start MLflow experiment
        mlflow.set_experiment('dynamic-pricing-retraining')
        
        models_to_try = {
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
            'linear_regression': LinearRegression()
        }
        
        best_model = None
        best_score = float('-inf')
        best_model_name = None
        model_results = {}
        
        for model_name, model in models_to_try.items():
            with mlflow.start_run(run_name=f'{model_name}_retrain_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'):
                print(f'Training {model_name}...')
                
                # Train model
                model.fit(X_train, y_train)
                
                # Make predictions
                y_pred = model.predict(X_test)
                
                # Calculate metrics
                mse = mean_squared_error(y_test, y_pred)
                rmse = np.sqrt(mse)
                mae = mean_absolute_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                
                # Cross-validation score
                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
                cv_mean = cv_scores.mean()
                
                print(f'{model_name} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}, CV R2: {cv_mean:.4f}')
                
                # Log metrics to MLflow
                mlflow.log_metrics({
                    'rmse': rmse,
                    'mae': mae,
                    'r2_score': r2,
                    'cv_r2_mean': cv_mean,
                    'cv_r2_std': cv_scores.std()
                })
                
                # Log model
                mlflow.sklearn.log_model(model, model_name)
                
                # Store results
                model_results[model_name] = {
                    'rmse': rmse,
                    'mae': mae,
                    'r2_score': r2,
                    'cv_r2_mean': cv_mean,
                    'model': model
                }
                
                # Check if this is the best model so far
                if cv_mean > best_score:
                    best_score = cv_mean
                    best_model = model
                    best_model_name = model_name
        
        print(f'Best model: {best_model_name} with CV R2 score: {best_score:.4f}')
        
        # Save best model
        model_version = f'challenger_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'
        model_dir = f'models/{model_version}'
        os.makedirs(model_dir, exist_ok=True)
        
        with open(f'{model_dir}/model.pkl', 'wb') as f:
            pickle.dump(best_model, f)
        
        # Save model metadata
        metadata = {
            'model_name': best_model_name,
            'model_version': model_version,
            'training_data_version': '${{ needs.check-retraining-triggers.outputs.data_version }}',
            'performance_metrics': model_results[best_model_name],
            'training_samples': len(df),
            'feature_columns': feature_cols,
            'created_at': datetime.now().isoformat()
        }
        
        with open(f'{model_dir}/metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f'Model saved to {model_dir}')
        
        # Set GitHub outputs
        print(f'::set-output name=model_version::{model_version}')
        print(f'::set-output name=model_performance::{best_score:.4f}')
        "
        
    - name: Upload model artifact
      uses: actions/upload-artifact@v3
      with:
        name: challenger-model-${{ steps.train-model.outputs.model_version }}
        path: models/
        retention-days: 90
        
    - name: Build challenger Docker image
      run: |
        # Create Dockerfile for challenger model
        cat > Dockerfile.challenger << EOF
        FROM python:3.9-slim
        
        WORKDIR /app
        
        COPY requirements.txt .
        RUN pip install -r requirements.txt
        
        COPY api/ ./api/
        COPY models/${{ steps.train-model.outputs.model_version }}/ ./models/current/
        
        EXPOSE 8000
        
        CMD [\"uvicorn\", \"api.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]
        EOF
        
        docker build -f Dockerfile.challenger -t challenger-model:${{ steps.train-model.outputs.model_version }} .
        
    - name: Deploy challenger model
      id: deploy-challenger
      run: |
        # In a real scenario, this would deploy to your cloud provider
        # For now, we'll simulate the deployment
        
        challenger_endpoint="http://dynamic-pricing-challenger-${{ github.run_number }}.eastus.azurecontainer.io:8000"
        
        echo "Deploying challenger model to: $challenger_endpoint"
        echo "Model version: ${{ steps.train-model.outputs.model_version }}"
        
        # Simulate deployment steps:
        # 1. Push Docker image to registry
        # 2. Deploy to Azure Container Instances or Kubernetes
        # 3. Configure load balancer for A/B testing
        
        echo "::set-output name=challenger_endpoint::$challenger_endpoint"

  setup-ab-test:
    name: Setup A/B Test
    runs-on: ubuntu-latest
    needs: [check-retraining-triggers, retrain-model]
    if: needs.check-retraining-triggers.outputs.should_retrain == 'true' && needs.retrain-model.result == 'success'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure A/B test routing
      run: |
        echo "Setting up A/B test configuration..."
        
        # Create A/B test configuration
        cat > ab-test-config.json << EOF
        {
          "experiment_id": "model-retraining-${{ github.run_number }}",
          "start_time": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "duration_days": 7,
          "traffic_split": {
            "champion": 90,
            "challenger": 10
          },
          "champion_endpoint": "http://dynamic-pricing-api.eastus.azurecontainer.io:8000",
          "challenger_endpoint": "${{ needs.retrain-model.outputs.challenger_endpoint }}",
          "success_metrics": [
            "conversion_rate",
            "revenue_per_user",
            "prediction_accuracy"
          ],
          "minimum_sample_size": 1000,
          "statistical_significance_threshold": 0.05
        }
        EOF
        
        echo "A/B test configuration:"
        cat ab-test-config.json
        
        # In a real scenario, this would:
        # 1. Update load balancer configuration
        # 2. Configure traffic routing rules
        # 3. Set up monitoring and alerting
        # 4. Initialize experiment tracking
        
    - name: Start A/B test monitoring
      run: |
        python -c "
        import json
        from datetime import datetime, timedelta
        
        print('Starting A/B test monitoring...')
        
        # Create monitoring configuration
        monitoring_config = {
            'experiment_id': 'model-retraining-${{ github.run_number }}',
            'monitoring_interval_minutes': 30,
            'alert_thresholds': {
                'error_rate_threshold': 0.05,
                'latency_threshold_ms': 1000,
                'traffic_split_deviation_threshold': 0.05
            },
            'auto_promotion_criteria': {
                'min_runtime_hours': 168,  # 7 days
                'min_samples_per_variant': 1000,
                'improvement_threshold': 0.02,
                'significance_level': 0.05
            }
        }
        
        print('Monitoring configuration set up successfully')
        print(f'Experiment will run for 7 days with 10% traffic to challenger')
        print(f'Auto-promotion will occur if challenger shows >2% improvement with 95% confidence')
        "

  notify-stakeholders:
    name: Notify Stakeholders
    runs-on: ubuntu-latest
    needs: [check-retraining-triggers, retrain-model, setup-ab-test]
    if: always()
    
    steps:
    - name: Notify on successful retraining
      if: needs.retrain-model.result == 'success'
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Model Retraining Completed Successfully - Run ${{ github.run_number }}`,
            body: `
            ## 🔄 Model Retraining Completed Successfully
            
            **Run Number:** ${{ github.run_number }}
            **Trigger Reason:** ${{ needs.check-retraining-triggers.outputs.trigger_reason }}
            **Timestamp:** ${new Date().toISOString()}
            
            ### What happened:
            - ✅ New training data collected
            - ✅ Multiple models retrained and evaluated
            - ✅ Best performing model selected as challenger
            - ✅ Challenger model deployed for A/B testing
            - ✅ A/B test monitoring started (10% traffic)
            
            ### Model Details:
            - **Model Version:** ${{ needs.retrain-model.outputs.model_version }}
            - **Performance Score:** ${{ needs.retrain-model.outputs.model_performance }}
            - **Data Version:** ${{ needs.check-retraining-triggers.outputs.data_version }}
            
            ### Next Steps:
            1. Monitor challenger performance for 7 days
            2. Compare metrics between champion and challenger
            3. Automatic promotion if challenger outperforms champion
            4. Manual review available at any time
            
            ### A/B Test Details:
            - **Duration:** 7 days
            - **Traffic Split:** 90% Champion, 10% Challenger
            - **Challenger Endpoint:** ${{ needs.retrain-model.outputs.challenger_endpoint }}
            
            The system will automatically evaluate and promote the challenger if it shows significant improvement.
            `,
            labels: ['ml-ops', 'retraining', 'success', 'ab-test']
          })
    
    - name: Notify on retraining failure
      if: needs.retrain-model.result == 'failure'
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Model Retraining Failed - Run ${{ github.run_number }}`,
            body: `
            ## ❌ Model Retraining Failed
            
            **Run Number:** ${{ github.run_number }}
            **Trigger Reason:** ${{ needs.check-retraining-triggers.outputs.trigger_reason }}
            **Timestamp:** ${new Date().toISOString()}
            
            The automated retraining pipeline has failed. Please review the logs and take corrective action.
            
            [View Workflow Run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            ### Possible Issues:
            - Data collection failure
            - Model training errors
            - Infrastructure problems
            - Configuration issues
            
            **Action Required:** Manual investigation and fix needed.
            `,
            labels: ['ml-ops', 'retraining', 'failure', 'urgent']
          })
    
    - name: Notify if no retraining needed
      if: needs.check-retraining-triggers.outputs.should_retrain == 'false'
      uses: actions/github-script@v6
      with:
        script: |
          console.log('No retraining triggers detected. Model is up to date.');
          
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Model Retraining Check - No Action Needed - Run ${{ github.run_number }}`,
            body: `
            ## ℹ️ Model Retraining Check Completed
            
            **Run Number:** ${{ github.run_number }}
            **Timestamp:** ${new Date().toISOString()}
            
            The automated retraining pipeline checked for retraining triggers but found none.
            The current model is still performing within acceptable parameters.
            
            ### What was checked:
            - ✅ Data freshness
            - ✅ Model performance metrics
            - ✅ Data drift indicators
            - ✅ External triggers
            
            **Result:** No retraining needed at this time.
            
            The next scheduled check will run according to the cron schedule or when triggered manually.
            `,
            labels: ['ml-ops', 'retraining', 'info']
          })